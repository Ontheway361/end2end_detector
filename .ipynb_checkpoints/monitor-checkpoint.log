----------------Environment Versions----------------
- Python: 3.7.3 
- PyTorch: 1.1.0
- TorchVison: 0.3.0
- device: True
----------------------------------------------------
Parallel mode was going ...
Model loading was finished ...
After data augmentation, 12730 rows added.
After data augmentation,   0 rows added.
Data loading was finished ...
epoch :  1|21, iter :  60|639,  loss : 1.7818
epoch :  1|21, iter : 120|639,  loss : 1.2761
epoch :  1|21, iter : 180|639,  loss : 1.0355
epoch :  1|21, iter : 240|639,  loss : 0.8897
epoch :  1|21, iter : 300|639,  loss : 0.7945
epoch :  1|21, iter : 360|639,  loss : 0.7180
epoch :  1|21, iter : 420|639,  loss : 0.6602
epoch :  1|21, iter : 480|639,  loss : 0.6123
epoch :  1|21, iter : 540|639,  loss : 0.5726
epoch :  1|21, iter : 600|639,  loss : 0.5389
acc : 0.8700, precision : 0.5952, recall : 0.8849, f1_score : 0.7117
train_loss : 0.5198
eval_loss : 0.2196
acc : 0.9234, precision : 0.5914, recall : 0.9390, f1_score : 0.7257
------------------------------------ gt vs. pred ------------------------------------
              precision    recall  f1-score   support

           0     0.9266    0.8524    0.8879      9226
           1     0.9355    0.9694    0.9521     20372

    accuracy                         0.9329     29598
   macro avg     0.9310    0.9109    0.9200     29598
weighted avg     0.9327    0.9329    0.9321     29598

[[ 7864  1362]
 [  623 19749]]
-------------------------------------------------------------------------------------
              precision    recall  f1-score   support

           0     0.9881    0.9954    0.9918      7434
           1     0.9899    0.9951    0.9925      7422
           2     0.9975    0.9836    0.9905      7433
           3     0.9928    0.9941    0.9934      7309

    accuracy                         0.9921     29598
   macro avg     0.9921    0.9921    0.9921     29598
weighted avg     0.9921    0.9921    0.9921     29598

[[7400   17    8    9]
 [   6 7386    3   27]
 [  67   38 7311   17]
 [  16   20    7 7266]]
-------------------------------------------------------------------------------------
Single epoch cost time : 27.84 mins
****************new SOTA was found****************
epoch :  2|21, iter :  60|639,  loss : 0.2124
epoch :  2|21, iter : 120|639,  loss : 0.2061
epoch :  2|21, iter : 180|639,  loss : 0.2034
epoch :  2|21, iter : 240|639,  loss : 0.1996
epoch :  2|21, iter : 300|639,  loss : 0.1967
epoch :  2|21, iter : 360|639,  loss : 0.1922
epoch :  2|21, iter : 420|639,  loss : 0.1891
epoch :  2|21, iter : 480|639,  loss : 0.1862
epoch :  2|21, iter : 540|639,  loss : 0.1842
epoch :  2|21, iter : 600|639,  loss : 0.1826
acc : 0.9272, precision : 0.7295, recall : 0.9514, f1_score : 0.8258
train_loss : 0.1814
eval_loss : 0.2235
acc : 0.9204, precision : 0.5780, recall : 0.9743, f1_score : 0.7256
------------------------------------ gt vs. pred ------------------------------------
              precision    recall  f1-score   support

           0     0.9418    0.9191    0.9303      9226
           1     0.9638    0.9743    0.9690     20372

    accuracy                         0.9571     29598
   macro avg     0.9528    0.9467    0.9497     29598
weighted avg     0.9569    0.9571    0.9569     29598

[[ 8480   746]
 [  524 19848]]
-------------------------------------------------------------------------------------
              precision    recall  f1-score   support

           0     0.9134    0.9993    0.9545      7434
           1     0.9944    0.9853    0.9898      7422
           2     0.9974    0.9229    0.9587      7433
           3     0.9981    0.9877    0.9928      7309

    accuracy                         0.9737     29598
   macro avg     0.9758    0.9738    0.9740     29598
weighted avg     0.9757    0.9737    0.9739     29598

[[7429    1    3    1]
 [  96 7313    8    5]
 [ 563    2 6860    8]
 [  45   38    7 7219]]
-------------------------------------------------------------------------------------
Single epoch cost time : 4.19 mins
epoch :  3|21, iter :  60|639,  loss : 0.1469
epoch :  3|21, iter : 120|639,  loss : 0.1422
epoch :  3|21, iter : 180|639,  loss : 0.1435
epoch :  3|21, iter : 240|639,  loss : 0.1454
epoch :  3|21, iter : 300|639,  loss : 0.1479
epoch :  3|21, iter : 360|639,  loss : 0.1464
epoch :  3|21, iter : 420|639,  loss : 0.1483
epoch :  3|21, iter : 480|639,  loss : 0.1475
epoch :  3|21, iter : 540|639,  loss : 0.1480
epoch :  3|21, iter : 600|639,  loss : 0.1485
acc : 0.9308, precision : 0.7374, recall : 0.9606, f1_score : 0.8343
train_loss : 0.1483
eval_loss : 0.1726
acc : 0.9245, precision : 0.5928, recall : 0.9584, f1_score : 0.7325
------------------------------------ gt vs. pred ------------------------------------
              precision    recall  f1-score   support

           0     0.9502    0.9369    0.9435      9226
           1     0.9716    0.9778    0.9747     20372

    accuracy                         0.9650     29598
   macro avg     0.9609    0.9573    0.9591     29598
weighted avg     0.9649    0.9650    0.9650     29598

[[ 8644   582]
 [  453 19919]]
-------------------------------------------------------------------------------------
              precision    recall  f1-score   support

           0     0.9832    0.9983    0.9907      7434
           1     0.9989    0.9580    0.9780      7422
           2     0.9950    0.9861    0.9905      7433
           3     0.9619    0.9956    0.9785      7309

    accuracy                         0.9845     29598
   macro avg     0.9847    0.9845    0.9844     29598
weighted avg     0.9848    0.9845    0.9844     29598

[[7421    2    7    4]
 [  20 7110   15  277]
 [  94    2 7330    7]
 [  13    4   15 7277]]
-------------------------------------------------------------------------------------
Single epoch cost time : 4.48 mins
****************new SOTA was found****************
epoch :  4|21, iter :  60|639,  loss : 0.1202
epoch :  4|21, iter : 120|639,  loss : 0.1368
epoch :  4|21, iter : 180|639,  loss : 0.1367
epoch :  4|21, iter : 240|639,  loss : 0.1369
epoch :  4|21, iter : 300|639,  loss : 0.1390
epoch :  4|21, iter : 360|639,  loss : 0.1366
epoch :  4|21, iter : 420|639,  loss : 0.1384
epoch :  4|21, iter : 480|639,  loss : 0.1388
epoch :  4|21, iter : 540|639,  loss : 0.1399
epoch :  4|21, iter : 600|639,  loss : 0.1394
acc : 0.9318, precision : 0.7395, recall : 0.9629, f1_score : 0.8365
train_loss : 0.1391
eval_loss : 0.1431
acc : 0.9281, precision : 0.6057, recall : 0.9571, f1_score : 0.7419
------------------------------------ gt vs. pred ------------------------------------
              precision    recall  f1-score   support

           0     0.9175    0.9649    0.9406      9226
           1     0.9837    0.9607    0.9721     20372

    accuracy                         0.9620     29598
   macro avg     0.9506    0.9628    0.9564     29598
weighted avg     0.9631    0.9620    0.9623     29598

[[ 8902   324]
 [  800 19572]]
-------------------------------------------------------------------------------------
              precision    recall  f1-score   support

           0     0.9949    0.9938    0.9943      7434
           1     0.9981    0.9931    0.9956      7422
           2     0.9981    0.9896    0.9939      7433
           3     0.9842    0.9988    0.9914      7309

    accuracy                         0.9938     29598
   macro avg     0.9938    0.9938    0.9938     29598
weighted avg     0.9939    0.9938    0.9938     29598

[[7388    4    9   33]
 [   3 7371    4   44]
 [  32    5 7356   40]
 [   3    5    1 7300]]
-------------------------------------------------------------------------------------
Single epoch cost time : 4.32 mins
****************new SOTA was found****************
epoch :  5|21, iter :  60|639,  loss : 0.1284
epoch :  5|21, iter : 120|639,  loss : 0.1317
epoch :  5|21, iter : 180|639,  loss : 0.1307
epoch :  5|21, iter : 240|639,  loss : 0.1342
epoch :  5|21, iter : 300|639,  loss : 0.1350
epoch :  5|21, iter : 360|639,  loss : 0.1358
epoch :  5|21, iter : 420|639,  loss : 0.1350
epoch :  5|21, iter : 480|639,  loss : 0.1354
epoch :  5|21, iter : 540|639,  loss : 0.1341
epoch :  5|21, iter : 600|639,  loss : 0.1348
acc : 0.9316, precision : 0.7392, recall : 0.9622, f1_score : 0.8361
train_loss : 0.1352
eval_loss : 0.1243
acc : 0.9258, precision : 0.5967, recall : 0.9634, f1_score : 0.7370
------------------------------------ gt vs. pred ------------------------------------
              precision    recall  f1-score   support

           0     0.9434    0.9481    0.9457      9226
           1     0.9764    0.9742    0.9753     20372

    accuracy                         0.9661     29598
   macro avg     0.9599    0.9612    0.9605     29598
weighted avg     0.9661    0.9661    0.9661     29598

[[ 8747   479]
 [  525 19847]]
-------------------------------------------------------------------------------------
              precision    recall  f1-score   support

           0     0.9956    0.9960    0.9958      7434
           1     0.9921    0.9973    0.9947      7422
           2     0.9974    0.9946    0.9960      7433
           3     0.9963    0.9934    0.9949      7309

    accuracy                         0.9953     29598
   macro avg     0.9953    0.9953    0.9953     29598
weighted avg     0.9953    0.9953    0.9953     29598

[[7404    9   12    9]
 [   4 7402    5   11]
 [  22   11 7393    7]
 [   7   39    2 7261]]
-------------------------------------------------------------------------------------
Single epoch cost time : 4.34 mins
****************new SOTA was found****************
epoch :  6|21, iter :  60|639,  loss : 0.1321
epoch :  6|21, iter : 120|639,  loss : 0.1247
epoch :  6|21, iter : 180|639,  loss : 0.1253
epoch :  6|21, iter : 240|639,  loss : 0.1269
epoch :  6|21, iter : 300|639,  loss : 0.1289
epoch :  6|21, iter : 360|639,  loss : 0.1300
epoch :  6|21, iter : 420|639,  loss : 0.1306
epoch :  6|21, iter : 480|639,  loss : 0.1295
epoch :  6|21, iter : 540|639,  loss : 0.1291
epoch :  6|21, iter : 600|639,  loss : 0.1281
acc : 0.9318, precision : 0.7404, recall : 0.9606, f1_score : 0.8363
train_loss : 0.1285
eval_loss : 0.1708
acc : 0.9255, precision : 0.6012, recall : 0.9192, f1_score : 0.7270
------------------------------------ gt vs. pred ------------------------------------
              precision    recall  f1-score   support

           0     0.9052    0.9367    0.9207      9226
           1     0.9709    0.9556    0.9632     20372

    accuracy                         0.9497     29598
   macro avg     0.9380    0.9461    0.9419     29598
weighted avg     0.9504    0.9497    0.9499     29598

[[ 8642   584]
 [  905 19467]]
-------------------------------------------------------------------------------------
              precision    recall  f1-score   support

           0     0.9918    0.9945    0.9931      7434
           1     0.9968    0.9961    0.9964      7422
           2     0.9965    0.9915    0.9940      7433
           3     0.9940    0.9970    0.9955      7309

    accuracy                         0.9948     29598
   macro avg     0.9948    0.9948    0.9948     29598
weighted avg     0.9948    0.9948    0.9948     29598

[[7393    6   13   22]
 [   3 7393    9   17]
 [  54    4 7370    5]
 [   4   14    4 7287]]
-------------------------------------------------------------------------------------
Single epoch cost time : 4.33 mins
epoch :  7|21, iter :  60|639,  loss : 0.1054
epoch :  7|21, iter : 120|639,  loss : 0.1125
epoch :  7|21, iter : 180|639,  loss : 0.1160
epoch :  7|21, iter : 240|639,  loss : 0.1176
epoch :  7|21, iter : 300|639,  loss : 0.1207
epoch :  7|21, iter : 360|639,  loss : 0.1214
epoch :  7|21, iter : 420|639,  loss : 0.1244
epoch :  7|21, iter : 480|639,  loss : 0.1259
epoch :  7|21, iter : 540|639,  loss : 0.1259
epoch :  7|21, iter : 600|639,  loss : 0.1252
acc : 0.9330, precision : 0.7425, recall : 0.9648, f1_score : 0.8392
train_loss : 0.1245
eval_loss : 0.1104
acc : 0.9284, precision : 0.6046, recall : 0.9743, f1_score : 0.7462
------------------------------------ gt vs. pred ------------------------------------
              precision    recall  f1-score   support

           0     0.9488    0.9583    0.9535      9226
           1     0.9810    0.9766    0.9788     20372

    accuracy                         0.9709     29598
   macro avg     0.9649    0.9674    0.9662     29598
weighted avg     0.9710    0.9709    0.9709     29598

[[ 8841   385]
 [  477 19895]]
-------------------------------------------------------------------------------------
              precision    recall  f1-score   support

           0     0.9960    0.9984    0.9972      7434
           1     0.9991    0.9953    0.9972      7422
           2     0.9972    0.9980    0.9976      7433
           3     0.9973    0.9978    0.9975      7309

    accuracy                         0.9974     29598
   macro avg     0.9974    0.9974    0.9974     29598
weighted avg     0.9974    0.9974    0.9974     29598

[[7422    1   10    1]
 [  10 7387    9   16]
 [  10    2 7418    3]
 [  10    4    2 7293]]
-------------------------------------------------------------------------------------
Single epoch cost time : 4.23 mins
****************new SOTA was found****************
epoch :  8|21, iter :  60|639,  loss : 0.1161
epoch :  8|21, iter : 120|639,  loss : 0.1151
epoch :  8|21, iter : 180|639,  loss : 0.1189
epoch :  8|21, iter : 240|639,  loss : 0.1178
epoch :  8|21, iter : 300|639,  loss : 0.1179
epoch :  8|21, iter : 360|639,  loss : 0.1206
epoch :  8|21, iter : 420|639,  loss : 0.1223
epoch :  8|21, iter : 480|639,  loss : 0.1218
epoch :  8|21, iter : 540|639,  loss : 0.1219
epoch :  8|21, iter : 600|639,  loss : 0.1208
acc : 0.9331, precision : 0.7427, recall : 0.9658, f1_score : 0.8397
train_loss : 0.1207
eval_loss : 0.1505
acc : 0.9269, precision : 0.6023, recall : 0.9509, f1_score : 0.7375
------------------------------------ gt vs. pred ------------------------------------
              precision    recall  f1-score   support

           0     0.9087    0.9500    0.9289      9226
           1     0.9769    0.9568    0.9667     20372

    accuracy                         0.9547     29598
   macro avg     0.9428    0.9534    0.9478     29598
weighted avg     0.9556    0.9547    0.9549     29598

[[ 8765   461]
 [  881 19491]]
-------------------------------------------------------------------------------------
              precision    recall  f1-score   support

           0     0.9946    0.9980    0.9963      7434
           1     0.9964    0.9974    0.9969      7422
           2     0.9980    0.9933    0.9956      7433
           3     0.9963    0.9966    0.9964      7309

    accuracy                         0.9963     29598
   macro avg     0.9963    0.9963    0.9963     29598
weighted avg     0.9963    0.9963    0.9963     29598

[[7419    2    8    5]
 [   4 7403    4   11]
 [  33    6 7383   11]
 [   3   19    3 7284]]
-------------------------------------------------------------------------------------
Single epoch cost time : 4.17 mins
epoch :  9|21, iter :  60|639,  loss : 0.1105
epoch :  9|21, iter : 120|639,  loss : 0.1111
epoch :  9|21, iter : 180|639,  loss : 0.1067
epoch :  9|21, iter : 240|639,  loss : 0.1113
epoch :  9|21, iter : 300|639,  loss : 0.1135
epoch :  9|21, iter : 360|639,  loss : 0.1145
epoch :  9|21, iter : 420|639,  loss : 0.1147
epoch :  9|21, iter : 480|639,  loss : 0.1155
epoch :  9|21, iter : 540|639,  loss : 0.1156
epoch :  9|21, iter : 600|639,  loss : 0.1173
acc : 0.9327, precision : 0.7417, recall : 0.9649, f1_score : 0.8387
train_loss : 0.1173
eval_loss : 0.1090
acc : 0.9278, precision : 0.6026, recall : 0.9728, f1_score : 0.7442
------------------------------------ gt vs. pred ------------------------------------
              precision    recall  f1-score   support

           0     0.9540    0.9489    0.9515      9226
           1     0.9769    0.9793    0.9781     20372

    accuracy                         0.9698     29598
   macro avg     0.9655    0.9641    0.9648     29598
weighted avg     0.9698    0.9698    0.9698     29598

[[ 8755   471]
 [  422 19950]]
-------------------------------------------------------------------------------------
              precision    recall  f1-score   support

           0     0.9978    0.9966    0.9972      7434
           1     0.9985    0.9964    0.9974      7422
           2     0.9937    0.9983    0.9960      7433
           3     0.9984    0.9971    0.9977      7309

    accuracy                         0.9971     29598
   macro avg     0.9971    0.9971    0.9971     29598
weighted avg     0.9971    0.9971    0.9971     29598

[[7409    2   19    4]
 [   5 7395   18    4]
 [   7    2 7420    4]
 [   4    7   10 7288]]
-------------------------------------------------------------------------------------
Single epoch cost time : 4.16 mins
****************new SOTA was found****************
epoch : 10|21, iter :  60|639,  loss : 0.1082
epoch : 10|21, iter : 120|639,  loss : 0.1084
epoch : 10|21, iter : 180|639,  loss : 0.1085
epoch : 10|21, iter : 240|639,  loss : 0.1092
epoch : 10|21, iter : 300|639,  loss : 0.1109
epoch : 10|21, iter : 360|639,  loss : 0.1139
epoch : 10|21, iter : 420|639,  loss : 0.1127
epoch : 10|21, iter : 480|639,  loss : 0.1138
epoch : 10|21, iter : 540|639,  loss : 0.1135
epoch : 10|21, iter : 600|639,  loss : 0.1138
acc : 0.9333, precision : 0.7434, recall : 0.9655, f1_score : 0.8400
train_loss : 0.1147
eval_loss : 0.1110
acc : 0.9272, precision : 0.6031, recall : 0.9509, f1_score : 0.7381
------------------------------------ gt vs. pred ------------------------------------
              precision    recall  f1-score   support

           0     0.9387    0.9665    0.9524      9226
           1     0.9846    0.9714    0.9780     20372

    accuracy                         0.9699     29598
   macro avg     0.9617    0.9690    0.9652     29598
weighted avg     0.9703    0.9699    0.9700     29598

[[ 8917   309]
 [  582 19790]]
-------------------------------------------------------------------------------------
              precision    recall  f1-score   support

           0     0.9941    0.9981    0.9961      7434
           1     0.9970    0.9970    0.9970      7422
           2     0.9961    0.9957    0.9959      7433
           3     0.9989    0.9952    0.9971      7309

    accuracy                         0.9965     29598
   macro avg     0.9965    0.9965    0.9965     29598
weighted avg     0.9965    0.9965    0.9965     29598

[[7420    2    9    3]
 [   8 7400   13    1]
 [  24    4 7401    4]
 [  12   16    7 7274]]
-------------------------------------------------------------------------------------
Single epoch cost time : 4.17 mins
epoch : 11|21, iter :  60|639,  loss : 0.0926
epoch : 11|21, iter : 120|639,  loss : 0.0924
epoch : 11|21, iter : 180|639,  loss : 0.0910
epoch : 11|21, iter : 240|639,  loss : 0.0880
epoch : 11|21, iter : 300|639,  loss : 0.0881
epoch : 11|21, iter : 360|639,  loss : 0.0887
epoch : 11|21, iter : 420|639,  loss : 0.0888
epoch : 11|21, iter : 480|639,  loss : 0.0886
epoch : 11|21, iter : 540|639,  loss : 0.0879
epoch : 11|21, iter : 600|639,  loss : 0.0885
acc : 0.9360, precision : 0.7487, recall : 0.9738, f1_score : 0.8465
train_loss : 0.0883
eval_loss : 0.0964
acc : 0.9278, precision : 0.6051, recall : 0.9543, f1_score : 0.7406
------------------------------------ gt vs. pred ------------------------------------
              precision    recall  f1-score   support

           0     0.9513    0.9633    0.9572      9226
           1     0.9833    0.9777    0.9805     20372

    accuracy                         0.9732     29598
   macro avg     0.9673    0.9705    0.9688     29598
weighted avg     0.9733    0.9732    0.9732     29598

[[ 8887   339]
 [  455 19917]]
-------------------------------------------------------------------------------------
              precision    recall  f1-score   support

           0     0.9973    0.9983    0.9978      7434
           1     0.9977    0.9989    0.9983      7422
           2     0.9974    0.9977    0.9976      7433
           3     0.9996    0.9971    0.9984      7309

    accuracy                         0.9980     29598
   macro avg     0.9980    0.9980    0.9980     29598
weighted avg     0.9980    0.9980    0.9980     29598

[[7421    1   12    0]
 [   3 7414    5    0]
 [  12    2 7416    3]
 [   5   14    2 7288]]
-------------------------------------------------------------------------------------
Single epoch cost time : 4.14 mins
****************new SOTA was found****************
epoch : 12|21, iter :  60|639,  loss : 0.0747
epoch : 12|21, iter : 120|639,  loss : 0.0805
epoch : 12|21, iter : 180|639,  loss : 0.0839
epoch : 12|21, iter : 240|639,  loss : 0.0848
epoch : 12|21, iter : 300|639,  loss : 0.0849
epoch : 12|21, iter : 360|639,  loss : 0.0834
epoch : 12|21, iter : 420|639,  loss : 0.0852
epoch : 12|21, iter : 480|639,  loss : 0.0860
epoch : 12|21, iter : 540|639,  loss : 0.0865
epoch : 12|21, iter : 600|639,  loss : 0.0868
acc : 0.9362, precision : 0.7488, recall : 0.9753, f1_score : 0.8472
train_loss : 0.0868
eval_loss : 0.1044
acc : 0.9284, precision : 0.6039, recall : 0.9778, f1_score : 0.7467
------------------------------------ gt vs. pred ------------------------------------
              precision    recall  f1-score   support

           0     0.9543    0.9585    0.9564      9226
           1     0.9812    0.9792    0.9802     20372

    accuracy                         0.9728     29598
   macro avg     0.9678    0.9689    0.9683     29598
weighted avg     0.9728    0.9728    0.9728     29598

[[ 8843   383]
 [  423 19949]]
-------------------------------------------------------------------------------------
              precision    recall  f1-score   support

           0     0.9896    0.9983    0.9939      7434
           1     0.9981    0.9981    0.9981      7422
           2     0.9981    0.9895    0.9938      7433
           3     0.9978    0.9977    0.9977      7309

    accuracy                         0.9959     29598
   macro avg     0.9959    0.9959    0.9959     29598
weighted avg     0.9959    0.9959    0.9959     29598

[[7421    2    6    5]
 [   3 7408    4    7]
 [  72    2 7355    4]
 [   3   10    4 7292]]
-------------------------------------------------------------------------------------
Single epoch cost time : 4.17 mins
epoch : 13|21, iter :  60|639,  loss : 0.0817
epoch : 13|21, iter : 120|639,  loss : 0.0826
epoch : 13|21, iter : 180|639,  loss : 0.0822
epoch : 13|21, iter : 240|639,  loss : 0.0826
epoch : 13|21, iter : 300|639,  loss : 0.0806
epoch : 13|21, iter : 360|639,  loss : 0.0827
epoch : 13|21, iter : 420|639,  loss : 0.0831
epoch : 13|21, iter : 480|639,  loss : 0.0821
epoch : 13|21, iter : 540|639,  loss : 0.0825
epoch : 13|21, iter : 600|639,  loss : 0.0840
acc : 0.9361, precision : 0.7488, recall : 0.9745, f1_score : 0.8469
train_loss : 0.0854
eval_loss : 0.0991
acc : 0.9276, precision : 0.6008, recall : 0.9815, f1_score : 0.7453
------------------------------------ gt vs. pred ------------------------------------
              precision    recall  f1-score   support

           0     0.9553    0.9568    0.9560      9226
           1     0.9804    0.9797    0.9801     20372

    accuracy                         0.9726     29598
   macro avg     0.9679    0.9682    0.9680     29598
weighted avg     0.9726    0.9726    0.9726     29598

[[ 8827   399]
 [  413 19959]]
-------------------------------------------------------------------------------------
              precision    recall  f1-score   support

           0     0.9957    0.9987    0.9972      7434
           1     0.9985    0.9977    0.9981      7422
           2     0.9972    0.9969    0.9970      7433
           3     0.9986    0.9967    0.9977      7309

    accuracy                         0.9975     29598
   macro avg     0.9975    0.9975    0.9975     29598
weighted avg     0.9975    0.9975    0.9975     29598

[[7424    1    8    1]
 [   5 7405    6    6]
 [  18    2 7410    3]
 [   9    8    7 7285]]
-------------------------------------------------------------------------------------
Single epoch cost time : 4.10 mins
epoch : 14|21, iter :  60|639,  loss : 0.0714
epoch : 14|21, iter : 120|639,  loss : 0.0743
epoch : 14|21, iter : 180|639,  loss : 0.0766
epoch : 14|21, iter : 240|639,  loss : 0.0769
epoch : 14|21, iter : 300|639,  loss : 0.0800
epoch : 14|21, iter : 360|639,  loss : 0.0817
epoch : 14|21, iter : 420|639,  loss : 0.0819
epoch : 14|21, iter : 480|639,  loss : 0.0820
epoch : 14|21, iter : 540|639,  loss : 0.0828
epoch : 14|21, iter : 600|639,  loss : 0.0832
acc : 0.9366, precision : 0.7498, recall : 0.9757, f1_score : 0.8480
train_loss : 0.0835
eval_loss : 0.0951
acc : 0.9929, precision : 0.9832, recall : 0.9753, f1_score : 0.9793
------------------------------------ gt vs. pred ------------------------------------
              precision    recall  f1-score   support

           0     0.9537    0.9610    0.9573      9226
           1     0.9823    0.9789    0.9806     20372

    accuracy                         0.9733     29598
   macro avg     0.9680    0.9699    0.9690     29598
weighted avg     0.9734    0.9733    0.9733     29598

[[ 8866   360]
 [  430 19942]]
-------------------------------------------------------------------------------------
              precision    recall  f1-score   support

           0     0.9980    0.9983    0.9981      7434
           1     0.9974    0.9987    0.9980      7422
           2     0.9974    0.9977    0.9976      7433
           3     0.9990    0.9973    0.9982      7309

    accuracy                         0.9980     29598
   macro avg     0.9980    0.9980    0.9980     29598
weighted avg     0.9980    0.9980    0.9980     29598

[[7421    1   12    0]
 [   2 7412    4    4]
 [  10    4 7416    3]
 [   3   14    3 7289]]
-------------------------------------------------------------------------------------
Single epoch cost time : 4.13 mins
****************new SOTA was found****************
epoch : 15|21, iter :  60|639,  loss : 0.0812
epoch : 15|21, iter : 120|639,  loss : 0.0779
epoch : 15|21, iter : 180|639,  loss : 0.0771
epoch : 15|21, iter : 240|639,  loss : 0.0781
epoch : 15|21, iter : 300|639,  loss : 0.0789
epoch : 15|21, iter : 360|639,  loss : 0.0800
epoch : 15|21, iter : 420|639,  loss : 0.0793
epoch : 15|21, iter : 480|639,  loss : 0.0797
epoch : 15|21, iter : 540|639,  loss : 0.0804
epoch : 15|21, iter : 600|639,  loss : 0.0808
acc : 0.9367, precision : 0.7501, recall : 0.9759, f1_score : 0.8482
train_loss : 0.0811
eval_loss : 0.0978
acc : 0.9278, precision : 0.6018, recall : 0.9793, f1_score : 0.7455
------------------------------------ gt vs. pred ------------------------------------
              precision    recall  f1-score   support

           0     0.9642    0.9494    0.9567      9226
           1     0.9772    0.9840    0.9806     20372

    accuracy                         0.9732     29598
   macro avg     0.9707    0.9667    0.9687     29598
weighted avg     0.9732    0.9732    0.9732     29598

[[ 8759   467]
 [  325 20047]]
-------------------------------------------------------------------------------------
              precision    recall  f1-score   support

           0     0.9972    0.9984    0.9978      7434
           1     0.9969    0.9987    0.9978      7422
           2     0.9973    0.9976    0.9974      7433
           3     0.9992    0.9959    0.9975      7309

    accuracy                         0.9976     29598
   macro avg     0.9976    0.9976    0.9976     29598
weighted avg     0.9976    0.9976    0.9976     29598

[[7422    1   10    1]
 [   3 7412    5    2]
 [  12    3 7415    3]
 [   6   19    5 7279]]
-------------------------------------------------------------------------------------
Single epoch cost time : 4.12 mins
epoch : 16|21, iter :  60|639,  loss : 0.0686
epoch : 16|21, iter : 120|639,  loss : 0.0665
epoch : 16|21, iter : 180|639,  loss : 0.0641
epoch : 16|21, iter : 240|639,  loss : 0.0613
epoch : 16|21, iter : 300|639,  loss : 0.0592
epoch : 16|21, iter : 360|639,  loss : 0.0596
epoch : 16|21, iter : 420|639,  loss : 0.0598
epoch : 16|21, iter : 480|639,  loss : 0.0606
epoch : 16|21, iter : 540|639,  loss : 0.0612
epoch : 16|21, iter : 600|639,  loss : 0.0617
acc : 0.9387, precision : 0.7540, recall : 0.9829, f1_score : 0.8534
train_loss : 0.0631
eval_loss : 0.0967
acc : 0.9281, precision : 0.6067, recall : 0.9484, f1_score : 0.7400
------------------------------------ gt vs. pred ------------------------------------
              precision    recall  f1-score   support

           0     0.9479    0.9654    0.9566      9226
           1     0.9842    0.9759    0.9801     20372

    accuracy                         0.9727     29598
   macro avg     0.9660    0.9707    0.9683     29598
weighted avg     0.9729    0.9727    0.9727     29598

[[ 8907   319]
 [  490 19882]]
-------------------------------------------------------------------------------------
              precision    recall  f1-score   support

           0     0.9983    0.9987    0.9985      7434
           1     0.9974    0.9989    0.9982      7422
           2     0.9976    0.9984    0.9980      7433
           3     0.9996    0.9969    0.9982      7309

    accuracy                         0.9982     29598
   macro avg     0.9982    0.9982    0.9982     29598
weighted avg     0.9982    0.9982    0.9982     29598

[[7424    1    9    0]
 [   3 7414    5    0]
 [   7    2 7421    3]
 [   3   16    4 7286]]
-------------------------------------------------------------------------------------
Single epoch cost time : 4.15 mins
epoch : 17|21, iter :  60|639,  loss : 0.0497
epoch : 17|21, iter : 120|639,  loss : 0.0509
epoch : 17|21, iter : 180|639,  loss : 0.0529
epoch : 17|21, iter : 240|639,  loss : 0.0569
epoch : 17|21, iter : 300|639,  loss : 0.0570
epoch : 17|21, iter : 360|639,  loss : 0.0576
epoch : 17|21, iter : 420|639,  loss : 0.0572
epoch : 17|21, iter : 480|639,  loss : 0.0568
epoch : 17|21, iter : 540|639,  loss : 0.0566
epoch : 17|21, iter : 600|639,  loss : 0.0565
acc : 0.9394, precision : 0.7555, recall : 0.9844, f1_score : 0.8549
train_loss : 0.0565
eval_loss : 0.0984
acc : 0.9291, precision : 0.6083, recall : 0.9643, f1_score : 0.7460
------------------------------------ gt vs. pred ------------------------------------
              precision    recall  f1-score   support

           0     0.9567    0.9586    0.9577      9226
           1     0.9812    0.9804    0.9808     20372

    accuracy                         0.9736     29598
   macro avg     0.9690    0.9695    0.9692     29598
weighted avg     0.9736    0.9736    0.9736     29598

[[ 8844   382]
 [  400 19972]]
-------------------------------------------------------------------------------------
              precision    recall  f1-score   support

           0     0.9976    0.9992    0.9984      7434
           1     0.9981    0.9985    0.9983      7422
           2     0.9983    0.9977    0.9980      7433
           3     0.9990    0.9975    0.9983      7309

    accuracy                         0.9982     29598
   macro avg     0.9982    0.9982    0.9982     29598
weighted avg     0.9982    0.9982    0.9982     29598

[[7428    1    5    0]
 [   3 7411    4    4]
 [  11    3 7416    3]
 [   4   10    4 7291]]
-------------------------------------------------------------------------------------
Single epoch cost time : 4.14 mins
epoch : 18|21, iter :  60|639,  loss : 0.0531
epoch : 18|21, iter : 120|639,  loss : 0.0520
epoch : 18|21, iter : 180|639,  loss : 0.0501
epoch : 18|21, iter : 240|639,  loss : 0.0494
epoch : 18|21, iter : 300|639,  loss : 0.0507
epoch : 18|21, iter : 360|639,  loss : 0.0520
epoch : 18|21, iter : 420|639,  loss : 0.0522
epoch : 18|21, iter : 480|639,  loss : 0.0525
epoch : 18|21, iter : 540|639,  loss : 0.0527
epoch : 18|21, iter : 600|639,  loss : 0.0532
acc : 0.9395, precision : 0.7556, recall : 0.9847, f1_score : 0.8551
train_loss : 0.0533
eval_loss : 0.0988
acc : 0.9284, precision : 0.6074, recall : 0.9534, f1_score : 0.7420
------------------------------------ gt vs. pred ------------------------------------
              precision    recall  f1-score   support

           0     0.9387    0.9755    0.9567      9226
           1     0.9887    0.9711    0.9798     20372

    accuracy                         0.9725     29598
   macro avg     0.9637    0.9733    0.9683     29598
weighted avg     0.9731    0.9725    0.9726     29598

[[ 9000   226]
 [  588 19784]]
-------------------------------------------------------------------------------------
              precision    recall  f1-score   support

           0     0.9977    0.9989    0.9983      7434
           1     0.9985    0.9980    0.9982      7422
           2     0.9984    0.9976    0.9980      7433
           3     0.9981    0.9982    0.9982      7309

    accuracy                         0.9982     29598
   macro avg     0.9982    0.9982    0.9982     29598
weighted avg     0.9982    0.9982    0.9982     29598

[[7426    1    6    1]
 [   2 7407    4    9]
 [  11    3 7415    4]
 [   4    7    2 7296]]
-------------------------------------------------------------------------------------
Single epoch cost time : 4.09 mins
epoch : 19|21, iter :  60|639,  loss : 0.0445
epoch : 19|21, iter : 120|639,  loss : 0.0406
epoch : 19|21, iter : 180|639,  loss : 0.0456
epoch : 19|21, iter : 240|639,  loss : 0.0467
epoch : 19|21, iter : 300|639,  loss : 0.0463
epoch : 19|21, iter : 360|639,  loss : 0.0481
epoch : 19|21, iter : 420|639,  loss : 0.0474
epoch : 19|21, iter : 480|639,  loss : 0.0478
epoch : 19|21, iter : 540|639,  loss : 0.0484
epoch : 19|21, iter : 600|639,  loss : 0.0492
acc : 0.9401, precision : 0.7570, recall : 0.9860, f1_score : 0.8565
train_loss : 0.0502
eval_loss : 0.1080
acc : 0.9279, precision : 0.6024, recall : 0.9778, f1_score : 0.7455
------------------------------------ gt vs. pred ------------------------------------
              precision    recall  f1-score   support

           0     0.9664    0.9403    0.9531      9226
           1     0.9733    0.9852    0.9792     20372

    accuracy                         0.9712     29598
   macro avg     0.9698    0.9627    0.9662     29598
weighted avg     0.9711    0.9712    0.9711     29598

[[ 8675   551]
 [  302 20070]]
-------------------------------------------------------------------------------------
              precision    recall  f1-score   support

           0     0.9980    0.9987    0.9983      7434
           1     0.9981    0.9987    0.9984      7422
           2     0.9977    0.9981    0.9979      7433
           3     0.9993    0.9977    0.9985      7309

    accuracy                         0.9983     29598
   macro avg     0.9983    0.9983    0.9983     29598
weighted avg     0.9983    0.9983    0.9983     29598

[[7424    1    9    0]
 [   3 7412    5    2]
 [   9    2 7419    3]
 [   3   11    3 7292]]
-------------------------------------------------------------------------------------
Single epoch cost time : 4.08 mins
epoch : 20|21, iter :  60|639,  loss : 0.0497
epoch : 20|21, iter : 120|639,  loss : 0.0478
epoch : 20|21, iter : 180|639,  loss : 0.0454
epoch : 20|21, iter : 240|639,  loss : 0.0446
epoch : 20|21, iter : 300|639,  loss : 0.0445
epoch : 20|21, iter : 360|639,  loss : 0.0457
epoch : 20|21, iter : 420|639,  loss : 0.0477
epoch : 20|21, iter : 480|639,  loss : 0.0480
epoch : 20|21, iter : 540|639,  loss : 0.0482
epoch : 20|21, iter : 600|639,  loss : 0.0468
acc : 0.9405, precision : 0.7579, recall : 0.9871, f1_score : 0.8575
train_loss : 0.0473
eval_loss : 0.1030
acc : 0.9291, precision : 0.6076, recall : 0.9684, f1_score : 0.7467
------------------------------------ gt vs. pred ------------------------------------
              precision    recall  f1-score   support

           0     0.9430    0.9719    0.9572      9226
           1     0.9871    0.9734    0.9802     20372

    accuracy                         0.9729     29598
   macro avg     0.9651    0.9727    0.9687     29598
weighted avg     0.9734    0.9729    0.9730     29598

[[ 8967   259]
 [  542 19830]]
-------------------------------------------------------------------------------------
              precision    recall  f1-score   support

           0     0.9984    0.9972    0.9978      7434
           1     0.9977    0.9991    0.9984      7422
           2     0.9965    0.9985    0.9975      7433
           3     0.9995    0.9973    0.9984      7309

    accuracy                         0.9980     29598
   macro avg     0.9980    0.9980    0.9980     29598
weighted avg     0.9980    0.9980    0.9980     29598

[[7413    1   19    1]
 [   2 7415    5    0]
 [   6    2 7422    3]
 [   4   14    2 7289]]
-------------------------------------------------------------------------------------
Single epoch cost time : 4.10 mins
epoch : 21|21, iter :  60|639,  loss : 0.0349
epoch : 21|21, iter : 120|639,  loss : 0.0394
epoch : 21|21, iter : 180|639,  loss : 0.0404
epoch : 21|21, iter : 240|639,  loss : 0.0393
epoch : 21|21, iter : 300|639,  loss : 0.0389
epoch : 21|21, iter : 360|639,  loss : 0.0412
epoch : 21|21, iter : 420|639,  loss : 0.0418
epoch : 21|21, iter : 480|639,  loss : 0.0432
epoch : 21|21, iter : 540|639,  loss : 0.0434
epoch : 21|21, iter : 600|639,  loss : 0.0435
acc : 0.9408, precision : 0.7583, recall : 0.9888, f1_score : 0.8583
train_loss : 0.0441
eval_loss : 0.1104
acc : 0.9289, precision : 0.6069, recall : 0.9693, f1_score : 0.7464
------------------------------------ gt vs. pred ------------------------------------
              precision    recall  f1-score   support

           0     0.9518    0.9620    0.9569      9226
           1     0.9827    0.9780    0.9803     20372

    accuracy                         0.9730     29598
   macro avg     0.9673    0.9700    0.9686     29598
weighted avg     0.9731    0.9730    0.9730     29598

[[ 8875   351]
 [  449 19923]]
-------------------------------------------------------------------------------------
              precision    recall  f1-score   support

           0     0.9972    0.9984    0.9978      7434
           1     0.9973    0.9989    0.9981      7422
           2     0.9980    0.9970    0.9975      7433
           3     0.9992    0.9973    0.9982      7309

    accuracy                         0.9979     29598
   macro avg     0.9979    0.9979    0.9979     29598
weighted avg     0.9979    0.9979    0.9979     29598

[[7422    1    8    3]
 [   3 7414    5    0]
 [  15    4 7411    3]
 [   3   15    2 7289]]
-------------------------------------------------------------------------------------
Single epoch cost time : 4.10 mins
